{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "j-VRsdH08Vdm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/twcs.csv\", delimiter=\",\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "kpT-m37B8361"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "dvlwbBBh86Qg"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to perform one hot encoding\n",
        "def one_hot_encoding(text):\n",
        "  label_encoder = LabelEncoder()\n",
        "  label_encoded = label_encoder.fit_transform(text.split())\n",
        "  onehot_encoder = OneHotEncoder()\n",
        "  label_encoded = label_encoded.reshape(len(label_encoded), 1)\n",
        "  onehot_encoded = onehot_encoder.fit_transform(label_encoded)\n",
        "  return onehot_encoded"
      ],
      "metadata": {
        "id": "fqm9VgvAFe_T"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to perform encoding using BOW\n",
        "def bow_encoding(text):\n",
        "  bow = CountVectorizer()\n",
        "  bow_encoded = bow.fit_transform(text.split('. '))\n",
        "  return bow_encoded"
      ],
      "metadata": {
        "id": "JaYGnsGQNR24"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to perform encoding using BOW\n",
        "def bigram_bow_encoding(text):\n",
        "  bow = CountVectorizer(ngram_range=(2,2))\n",
        "  bow_encoded = bow.fit_transform(text.split('. '))\n",
        "  return bow_encoded"
      ],
      "metadata": {
        "id": "_zPNP-d8QDuc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram_bow_encoding(text):\n",
        "  bow = CountVectorizer(ngram_range=(3,3))\n",
        "  bow_encoded = bow.fit_transform(text.split('. '))\n",
        "  return bow_encoded"
      ],
      "metadata": {
        "id": "jhL2O6rKQpZP"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_encoding(text):\n",
        "  tfidf = TfidfVectorizer()\n",
        "  tfidf_encoded = tfidf.fit_transform(text.split('. '))\n",
        "  return tfidf_encoded"
      ],
      "metadata": {
        "id": "dv1Zwa5ORXUk"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoding_functions = {\n",
        "    one_hot_encoding: \"One Hot Encoding\",\n",
        "    bow_encoding: \"Bag-of-words Encoding\",\n",
        "    bigram_bow_encoding: \"Bag-of-words Encoding(Bigram)\",\n",
        "    trigram_bow_encoding: \"Bag-of-words Encoding(Trigram)\",\n",
        "    tfidf_encoding: \"TF-IDF Encoding\"\n",
        "}"
      ],
      "metadata": {
        "id": "DAqyS8jSTKmL"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_encoding_function in text_encoding_functions.keys():\n",
        "  data_sample = data.head(3)\n",
        "  data_sample[\"text\"] = data_sample[\"text\"].astype(str).apply(text_encoding_function)\n",
        "  print(text_encoding_functions[text_encoding_function])\n",
        "  print(\"________________________________\")\n",
        "  print(data_sample[\"text\"][0].toarray())\n",
        "  print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJtuYyt3Thml",
        "outputId": "45c99324-a9f1-4fc4-ec94-81ce2c157361"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Hot Encoding\n",
            "________________________________\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "Bag-of-words Encoding\n",
            "________________________________\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 0 1 1]\n",
            " [0 1 1 1 1 0 1 1 1 1 2 0 1 1 1]]\n",
            "\n",
            "\n",
            "Bag-of-words Encoding(Bigram)\n",
            "________________________________\n",
            "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1]]\n",
            "\n",
            "\n",
            "Bag-of-words Encoding(Trigram)\n",
            "________________________________\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [1 1 0 1 1 1 1 0 1 1 1 0 1 1]]\n",
            "\n",
            "\n",
            "TF-IDF Encoding\n",
            "________________________________\n",
            "[[0.70710678 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.41779577 0.         0.         0.         0.54935123\n",
            "  0.         0.         0.         0.         0.41779577 0.\n",
            "  0.         0.41779577 0.41779577]\n",
            " [0.         0.21909986 0.28808999 0.28808999 0.28808999 0.\n",
            "  0.28808999 0.28808999 0.28808999 0.28808999 0.43819973 0.\n",
            "  0.28808999 0.21909986 0.21909986]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**One hot encoding**\n",
        "\n",
        "Advantages-\n",
        "\n",
        "\n",
        "*   One-hot encoding is used in NLP to encode categorical factors as binary vectors, such as words or part-of-speech identifiers\n",
        "*   This approach is helpful because machine learning algorithms generally act on numerical data, so representing text data as numerical vectors are required for these algorithms to work.\n",
        "*   In a sentiment analysis assignment, for example, we might describe each word in a sentence as a one-hot encoded vector and then use these vectors as input to a neural network to forecast the sentiment of the sentence.\n",
        "\n",
        "Disadvantages-\n",
        "\n",
        "\n",
        "\n",
        "*   One of the major disadvantages of one-hot encoding in NLP is that it produces high-dimensional sparse vectors that can be extremely costly to process\n",
        "*   Furthermore, because one-hot encoding does not catch the semantic connections between words, machine-learning models that use these vectors as input may perform poorly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5ePWgIq6cZNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of words(BOW)**\n",
        "\n",
        "Advantages-\n",
        "\n",
        "\n",
        "*   BoW is easy to understand and implement, making it a great starting point for text analysis.\n",
        "\n",
        "\n",
        "*   It can be used in various NLP tasks like sentiment analysis, document classification, and information retrieval.\n",
        "*   BoW treats each word independently, making it scalable and efficient for large datasets\n",
        "\n",
        "Disadvantages-\n",
        "\n",
        "\n",
        "*   It ignores the order and context of words, losing valuable information about sentence structure and semantics.\n",
        "\n",
        "\n",
        "*   For larger vocabularies or extensive text datasets, the resulting vectors can become extremely high-dimensional and sparse, impacting computational efficiency.\n",
        "*   Words not present in the vocabulary are disregarded, leading to information loss.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJBenMuJejUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-Grams**\n",
        "\n",
        "Advantages-\n",
        "\n",
        "\n",
        "*   The concept of n-grams is simple and easy to use yet powerful. Hence, it can be used to build a variety of applications in NLP, like language models, spelling correctors, etc.\n",
        "\n",
        "Disadvantages-\n",
        "\n",
        "\n",
        "*   N-grams cannot deal Out Of Vocabulary (OOV) words. It works well with the words present in the training set. In the case of an Out Of Vocabulary (OOV) word, n-grams fail to tackle it.\n",
        "*   Another serious concern about n-grams is that it deals with large sparsity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lTX0EWQsfQmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**\n",
        "\n",
        "Advantages-\n",
        "\n",
        "\n",
        "*   TF-IDF emphasizes the significance of terms that are crucial to individual documents but not necessarily common across the corpus.\n",
        "*   It’s adaptable to various NLP tasks such as information retrieval, text summarization, and keyword extraction\n",
        "\n",
        "\n",
        "*   TF-IDF is straightforward to implement and can quickly provide insights into the importance of words in a document.\n",
        "\n",
        "\n",
        "Disadvantages-\n",
        "\n",
        "\n",
        "*   In large datasets with numerous unique terms, the TF-IDF matrix can become extremely sparse, consuming more memory.\n",
        "\n",
        "\n",
        "*   It doesn’t consider the sequence or position of words within a document, potentially losing some contextual information.\n",
        "*   Depending on how stop words are handled, they might affect the TF-IDF scores disproportionately.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6b7ufAKwgMaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kAoSR9uKgHlr"
      }
    }
  ]
}